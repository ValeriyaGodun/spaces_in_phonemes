# Сегментация фонем японского языка с помощью HSMM

## Описание задачи

Цель: автоматически расставить пробелы в фонемной записи японского языка, используя информацию о расположении пробелов в обычной японской записи текста.

**Важная особенность:** несмотря на наличие небольшого размеченного датасета (около 5к предложений), данная задача решается как unsupervised (обучение без учителя). Модель должна самостоятельно выявить и усвоить общие закономерности соответствия между японскими символами и сегментами фонем.

## Датасет

Данные взяты из следующего репозитория:
https://github.com/Sveta23S/Japanese_spaces

**train.csv** - обучающий датасет, содержит 5874 строк

**Структура данных:**
- `id` - идентификатор предложения
- `phrase` - японская фраза без пробелов
- `split_phrase` - японская фраза с пробелами между словами
- `ipa` - фонемная запись без пробелов (IPA транскрипция)
- `split_ipa` - фонемная запись с пробелами между словами (маркер `<space>`)

**Пример данных:**

!!!картинка

## Подготовка данных

1. **Очистка:** удаление пробелов из колонок `ipa` и `split_ipa`
2. **Разбиение:** преобразование `split_phrase` в последовательность японских символов
3. **Анализ:** вычисление максимального отношения длины фонем к количеству символов для определения `max_chunk_len`

**Характеристики данных:**
- Всего предложений: 5874
- Всего уникальных японских символов: 1811
- Максимальная длина фонем: 32 символа
- Максимальное отношение длины фонем к числу символов: 4.67
- Среднее количество фонем на один японский символ: 2.234

!!!картинка

## Подход к решению

Для решения задачи используется **HSMM (Hidden Semi-Markov Model)** - скрытая полу-марковская модель, которая обучает распределение длины сегментов фонем для каждого японского символа (иероглифа).

### Ключевая идея

Модель решает задачу сегментации: для последовательности японских символов и соответствующей последовательности фонем необходимо определить, сколько фонем соответствует каждому символу. Это позволяет восстановить разметку `split_ipa` из `ipa` и `split_phrase`.

### Архитектура модели

**LengthHSMM** - модель, которая:
- Для каждого японского символа обучает распределение вероятностей длин сегментов фонем (от 1 до `max_len`)
- Использует алгоритм EM (Expectation-Maximization) для обучения
- Применяет алгоритм Витерби для декодирования наиболее вероятной сегментации

## Алгоритм обучения

### Инициализация параметров

1. **Жадная сегментация:** для каждого предложения выполняется начальная сегментация с равномерным распределением фонем по символам
2. **Подсчет частот:** для каждого символа подсчитываются частоты длин сегментов
3. **Нормализация:** частоты преобразуются в вероятности с добавлением априорного сглаживания (`length_prior`)

### Алгоритм EM

**E-шаг (Expectation):**
- Выполняются forward и backward проходы для вычисления апостериорных вероятностей
- Подсчитываются ожидаемые частоты длин сегментов для каждого символа

**M-шаг (Maximization):**
- Обновляются параметры модели (вероятности длин) на основе ожидаемых частот
- Применяется сглаживание для предотвращения нулевых вероятностей

### Декодирование

После обучения модель использует алгоритм Витерби для поиска наиболее вероятной последовательности длин сегментов.

## Гиперпараметры

```
max_len: 6 (определяется автоматически на основе данных как max_chunk_len)
length_prior: 0.2 (параметр сглаживания)
n_iter: 50 (количество итераций EM)
```

## Метрики оценки

### Boundary F1-score

Метрика оценивает точность определения границ между словами в фонемной записи. Для каждого предложения извлекаются позиции границ (места, где расположены маркеры `<space>`) из эталонной и предсказанной разметки. 
Метрика вычисляется как F1-score:
 `F1 = 2 * tp / (pred_total + true_total)`, где `tp` - количество правильно определенных границ (пересечение множеств границ), `pred_total` - общее количество предсказанных границ, `true_total` - общее количество эталонных границ.

## Результаты

Модель успешно обучается на данных и показывает стабильное улучшение log-likelihood в процессе обучения (от -1.6894 на первой итерации до -0.7278 на 50-й итерации). После обучения выполняется декодирование всех предложений с построением предсказаний `split_ipa_pred`. Для случаев, когда алгоритм Витерби не может найти корректную сегментацию, используется запасной вариант - равномерное распределение длин сегментов между символами.

**Boundary F1-score:** 0.8763

!!!картинка